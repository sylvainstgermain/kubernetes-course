You can read more about Project Calico here: https://docs.projectcalico.org/introduction/


kubectl apply -f objets-cours/...
kubectl config current-context
kubectl delete all --all

#Confirm namespace test exists.
kubectl get namespaces

#irst, we will define a Deployment named sise-deploy. We are doing this 
#in order to have a target to use while we are testing our routing. 
#This ultimately will be an endpoint for the service that we define 
#in the next step.

#You may already have a file with the same name but different data. If so,
 vim your file, then delete the data with the following keyboard strokes.
 # First run dShiftg. Then run dgg to delete everything from your cursor 
 #to the top.
---
apiVersion: apps/v1
kind: Deployment
metadata:
 name: sise-deploy
spec:
 replicas: 2
 selector:
   matchLabels:
     app: sise
 template:
   metadata:
     labels:
       app: sise
   spec:
     containers:
     - name: sise
       image: mhausenblas/simpleservice:0.5.0
       ports:
       - containerPort: 9876
       env:
       - name: SIMPLE_SERVICE_VERSION
         value: "1.0"

 #Note that the simpleservice container is designed as a testing container, 
 #and has the following paths available to retrieve information 
 #from: /health, /info, and /env.

#Next, let's create a manifest for a Service that will select all the pods 
#inside of our sise-deploy deployment via their label app: sise. Once running,
# we will be able to target this simpleservice and observe how it is acting 
#via the command calicoctl.

---
apiVersion: v1
kind: Service
metadata:
  name: simpleservice
spec:
  ports:
    - port: 80
      targetPort: 9876
  selector:
    app: sise

#Now create both the Service and the Deployment at the same time. Sometimes
 #it is nice and makes sense when you have multiple files that need created, and each has a relation to the other.

kubectl apply -f objets-cours/sise-svc.yaml -f objets-cours/sise-deploy.yaml

service/simpleservice created
deployment.apps/sise-deploy created

#Verify that the Pods have been created.
kubectl get pods
NAME                           READY   STATUS    RESTARTS   AGE
sise-deploy-7566bd957d-ql6qm   1/1     Running   0          25s
sise-deploy-7566bd957d-r2pcl   1/1     Running   0          25s

#Let's inspect the YAML of the Pods to see their generateName, nodeName, hostIP, and podIP.
kubectl get pods -o yaml | egrep " generateName:| nodeName:| hostIP:| podIP:"
 
  generateName: sise-deploy-7566bd957d-
    nodeName: node-2
    hostIP: 10.9.54.194
    podIP: 192.168.247.40
    generateName: sise-deploy-7566bd957d-
    nodeName: node-3
    hostIP: 10.8.79.30
    podIP: 192.168.139.103

#Next, let's verify that the service we initiated was indeed created.
kubectl get svc simpleservice
NAME            TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
simpleservice   ClusterIP   172.16.3.158   <none>        80/TCP    2m

=============================================
kow we know:

pod IPs
nodes they are resident on
svc (clusterIP) associated : 172.16.3.158
================================================

#Open up a new tmux pane, and ssh into any one of your masters
ssh master-1

#Although installing jq is not necessary to finish this lab,
# it does help to clean up the output.
# So let's do it and apt install jq.
sudo apt install jq -y

#Now that we have moved inside of our cluster to share the same ClusterIP 
#subnet, we are able to curl against the ClusterIP address of the 
#simpleservice Service. Replace <clusterIP> with the ClusterIP address 
#of your simpleservice, and curl all of the paths that will have 
#simpleservice retrieve information on.
curl -s 172.16.3.158/health | jq
{
  "healthy": true
}

curl -s 172.16.3.158/info | jq
{
  "host": "172.16.3.158",
  "version": "1.0",
  "from": "192.168.84.128"
}

curl -s 172.16.3.158/env | jq
{
  "version": "1.0",
  "env": "{'SIMPLESERVICE_PORT_80_TCP': 'tcp://172.16.3.158:80', 'KUBERNETES_PORT_443_TCP_ADDR': '172.16.3.1', 'HOME': '/root', 'PATH': '/usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'KUBERNETES_PORT_443_TCP': 'tcp://172.16.3.1:443', 'KUBERNETES_SERVICE_PORT': '443', 'LANG': 'C.UTF-8', 'PYTHON_VERSION': '2.7.13', 'KUBERNETES_SERVICE_HOST': '172.16.3.1', 'PYTHON_PIP_VERSION': '9.0.1', 'REFRESHED_AT': '2017-04-24T13:50', 'SIMPLESERVICE_PORT_80_TCP_PROTO': 'tcp', 'GPG_KEY': 'C01E1CAD5EA2C4F0B8E3571504C367C218ADD4FF', 'SIMPLESERVICE_SERVICE_HOST': '172.16.3.158', 'KUBERNETES_PORT': 'tcp://172.16.3.1:443', 'SIMPLESERVICE_PORT_80_TCP_ADDR': '172.16.3.158', 'KUBERNETES_SERVICE_PORT_HTTPS': '443', 'SIMPLE_SERVICE_VERSION': '1.0', 'KUBERNETES_PORT_443_TCP_PROTO': 'tcp', 'SIMPLESERVICE_PORT': 'tcp://172.16.3.158:80', 'SIMPLESERVICE_PORT_80_TCP_PORT': '80', 'HOSTNAME': 'sise-deploy-7566bd957d-r2pcl', 'KUBERNETES_PORT_443_TCP_PORT': '443', 'SIMPLESERVICE_SERVICE_PORT': '80'}"
}

Calico is already running as the network plugin for your cluster, but you are
 missing an important command line control service called calicoctl, 
 which will allow you to gather information about your cluster's already
existing Calico system. The following steps will guide you through the
install of calicoctl as a single pod, then how to use it to control your 
   Calico system. You will soon gain access to Calico's data store, with 
   full access and management privileges. Through this, you will gain a 
   clearer understanding of Calico's view of your cluster.

Leave the tmux pane of our master and RETURN TO THE BCHD TMUX PANE. Now, we will now install the CLI for Calico. There are several ways that you may install Calico. We have chosen to show you how to run Calico as a Pod from inside of your cluster. You can read more documentation about that process here. To start with, let's download the following manifest.








