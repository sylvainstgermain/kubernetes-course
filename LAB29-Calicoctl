You can read more about Project Calico here: https://docs.projectcalico.org/introduction/


kubectl apply -f objets-cours/...
kubectl config current-context
kubectl delete all --all

#Confirm namespace test exists.
kubectl get namespaces

#irst, we will define a Deployment named sise-deploy. We are doing this 
#in order to have a target to use while we are testing our routing. 
#This ultimately will be an endpoint for the service that we define 
#in the next step.

#You may already have a file with the same name but different data. If so,
 vim your file, then delete the data with the following keyboard strokes.
 # First run dShiftg. Then run dgg to delete everything from your cursor 
 #to the top.
---
apiVersion: apps/v1
kind: Deployment
metadata:
 name: sise-deploy
spec:
 replicas: 2
 selector:
   matchLabels:
     app: sise
 template:
   metadata:
     labels:
       app: sise
   spec:
     containers:
     - name: sise
       image: mhausenblas/simpleservice:0.5.0
       ports:
       - containerPort: 9876
       env:
       - name: SIMPLE_SERVICE_VERSION
         value: "1.0"

 #Note that the simpleservice container is designed as a testing container, 
 #and has the following paths available to retrieve information 
 #from: /health, /info, and /env.

#Next, let's create a manifest for a Service that will select all the pods 
#inside of our sise-deploy deployment via their label app: sise. Once running,
# we will be able to target this simpleservice and observe how it is acting 
#via the command calicoctl.

---
apiVersion: v1
kind: Service
metadata:
  name: simpleservice
spec:
  ports:
    - port: 80
      targetPort: 9876
  selector:
    app: sise

#Now create both the Service and the Deployment at the same time. Sometimes
 #it is nice and makes sense when you have multiple files that need created, and each has a relation to the other.

kubectl apply -f objets-cours/sise-svc.yaml -f objets-cours/sise-deploy.yaml

service/simpleservice created
deployment.apps/sise-deploy created

#Verify that the Pods have been created.
kubectl get pods
NAME                           READY   STATUS    RESTARTS   AGE
sise-deploy-7566bd957d-ql6qm   1/1     Running   0          25s
sise-deploy-7566bd957d-r2pcl   1/1     Running   0          25s

#Let's inspect the YAML of the Pods to see their generateName, nodeName, hostIP, and podIP.
kubectl get pods -o yaml | egrep " generateName:| nodeName:| hostIP:| podIP:"
 
  generateName: sise-deploy-7566bd957d-
    nodeName: node-2
    hostIP: 10.9.54.194
    podIP: 192.168.247.40
    generateName: sise-deploy-7566bd957d-
    nodeName: node-3
    hostIP: 10.8.79.30
    podIP: 192.168.139.103

#Next, let's verify that the service we initiated was indeed created.
kubectl get svc simpleservice
NAME            TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
simpleservice   ClusterIP   172.16.3.158   <none>        80/TCP    2m

=============================================
kow we know:

pod IPs
nodes they are resident on
svc (clusterIP) associated : 172.16.3.158
================================================

#Open up a new tmux pane, and ssh into any one of your masters
ssh master-1

#Although installing jq is not necessary to finish this lab,
# it does help to clean up the output.
# So let's do it and apt install jq.
sudo apt install jq -y

#Now that we have moved inside of our cluster to share the same ClusterIP 
#subnet, we are able to curl against the ClusterIP address of the 
#simpleservice Service. Replace <clusterIP> with the ClusterIP address 
#of your simpleservice, and curl all of the paths that will have 
#simpleservice retrieve information on.
curl -s 172.16.3.158/health | jq
{
  "healthy": true
}

curl -s 172.16.3.158/info | jq
{
  "host": "172.16.3.158",
  "version": "1.0",
  "from": "192.168.84.128"
}

curl -s 172.16.3.158/env | jq
{
  "version": "1.0",
  "env": "{'SIMPLESERVICE_PORT_80_TCP': 'tcp://172.16.3.158:80', 'KUBERNETES_PORT_443_TCP_ADDR': '172.16.3.1', 'HOME': '/root', 'PATH': '/usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'KUBERNETES_PORT_443_TCP': 'tcp://172.16.3.1:443', 'KUBERNETES_SERVICE_PORT': '443', 'LANG': 'C.UTF-8', 'PYTHON_VERSION': '2.7.13', 'KUBERNETES_SERVICE_HOST': '172.16.3.1', 'PYTHON_PIP_VERSION': '9.0.1', 'REFRESHED_AT': '2017-04-24T13:50', 'SIMPLESERVICE_PORT_80_TCP_PROTO': 'tcp', 'GPG_KEY': 'C01E1CAD5EA2C4F0B8E3571504C367C218ADD4FF', 'SIMPLESERVICE_SERVICE_HOST': '172.16.3.158', 'KUBERNETES_PORT': 'tcp://172.16.3.1:443', 'SIMPLESERVICE_PORT_80_TCP_ADDR': '172.16.3.158', 'KUBERNETES_SERVICE_PORT_HTTPS': '443', 'SIMPLE_SERVICE_VERSION': '1.0', 'KUBERNETES_PORT_443_TCP_PROTO': 'tcp', 'SIMPLESERVICE_PORT': 'tcp://172.16.3.158:80', 'SIMPLESERVICE_PORT_80_TCP_PORT': '80', 'HOSTNAME': 'sise-deploy-7566bd957d-r2pcl', 'KUBERNETES_PORT_443_TCP_PORT': '443', 'SIMPLESERVICE_SERVICE_PORT': '80'}"
}

Calico is already running as the network plugin for your cluster, but you are
 missing an important command line control service called calicoctl, 
 which will allow you to gather information about your cluster's already
existing Calico system. The following steps will guide you through the
install of calicoctl as a single pod, then how to use it to control your 
   Calico system. You will soon gain access to Calico's data store, with 
   full access and management privileges. Through this, you will gain a 
   clearer understanding of Calico's view of your cluster.

Leave the tmux pane of our master and RETURN TO THE BCHD TMUX PANE. 
Now, we will now install the CLI for Calico. There are several ways
 that you may install Calico. We have chosen to show you how to run 
 Calico as a Pod from inside of your cluster. You can read more documentation
  about that process here. To start with, let's download the following 
  manifest.
wget https://docs.projectcalico.org/manifests/calicoctl-etcd.yaml

Because we are using tls and etcd we will want to uncomment the env, 
volumeMounts, and volumes inside of the manifest we just retrieved. 
When you are done, it should look like the following.
# Calico Version v3.13.2
# https://docs.projectcalico.org/releases#v3.13.2
# This manifest includes the following component versions:
#   calico/ctl:v3.13.2

apiVersion: v1
kind: Pod
metadata:
  name: calicoctl
  namespace: kube-system
spec:
  nodeSelector:
    kubernetes.io/os: linux
  hostNetwork: true
  containers:
  - name: calicoctl
    image: calico/ctl:v3.13.2
    command: ["/bin/sh", "-c", "while true; do sleep 3600; done"]
    env:
    - name: ETCD_ENDPOINTS
      valueFrom:
        configMapKeyRef:
          name: calico-config
          key: etcd_endpoints
    # If you're using TLS enabled etcd uncomment the following.
    # Location of the CA certificate for etcd.
    - name: ETCD_CA_CERT_FILE
      valueFrom:
        configMapKeyRef:
          name: calico-config
          key: etcd_ca
    # Location of the client key for etcd.
    - name: ETCD_KEY_FILE
      valueFrom:
        configMapKeyRef:
          name: calico-config
          key: etcd_key
    # Location of the client certificate for etcd.
    - name: ETCD_CERT_FILE
      valueFrom:
        configMapKeyRef:
          name: calico-config
          key: etcd_cert
    volumeMounts:
    - mountPath: /calico-secrets
      name: etcd-certs
  volumes:
    # If you're using TLS enabled etcd uncomment the following.
    - name: etcd-certs
      secret:
        secretName: calico-etcd-secrets

Note: If you receive an error when trying to launch this Pod, 
you likely have indentation wrong. Try to fix it on your own, 
or feel free to copy the above manifest and overwrite what you downloaded.

#Initiate the calicoctl pod.
kubectl apply -f objets-cours/calicoctl-etcd.yaml

#Once the calicoctl pod is running, we will need to access it to run commands.
 If we had chosen a different installation option, we could have ran commands 
 locally to communicate with our cluster. However, there is a simple alias 
 that will allow us to still use the command calicoctl to run our commands.
  Set it up with the following line of code.

alias calicoctl="kubectl exec -ti -n kube-system calicoctl -- /calicoctl"
calicoctl --help
Usage:
  calicoctl [options] <command> [<args>...]

    create    Create a resource by filename or stdin.
    replace   Replace a resource by filename or stdin.
    apply     Apply a resource by filename or stdin.  This creates a resource
              if it does not exist, and replaces a resource if it does exists.
    patch     Patch a pre-exisiting resource in place.
    delete    Delete a resource identified by file, stdin or resource type and
              name.
    get       Get a resource identified by file, stdin or resource type and
              name.
    label     Add or update labels of resources.
    convert   Convert config files between different API versions.
    ipam      IP address management.
    node      Calico node management.
    version   Display the version of calicoctl.

Options:
  -h --help               Show this screen.
  -l --log-level=<level>  Set the log level (one of panic, fatal, error,
                          warn, info, debug) [default: panic]

#Make sure calicoctl is able to communicate with your entire cluster.
calicoctl get nodes
NAME     
node-1   
node-2   
node-3   

#To view more information about the nodes, let's use the -o wide flag.#
calicoctl get nodes -o wide
NAME     ASN       IPV4              IPV6   
node-1   (64512)   10.3.188.245/12          
node-2   (64512)   10.9.54.194/12           
node-3   (64512)   10.8.79.30/12    

#View the available range from which a Pod can be assigned an IP address, 
#by performing athe get ippools command.
calicoctl get ippools -o wide
NAME                  CIDR             NAT    IPIPMODE   VXLANMODE   DISABLED   SELECTOR   
default-ipv4-ippool   192.168.0.0/16   true   Always     Never       false      all()  


# Now to view the name of the interface that has been created which connects
# the Pod to the host machine, which is known as a workloadendpoint, 
# perform the following command.
calicoctl get workloadendpoints -o wide
NAME                                               WORKLOAD                       NODE     NETWORKS             INTERFACE         PROFILES                          NATS   
node--2-k8s-sise--deploy--7566bd957d--ql6qm-eth0   sise-deploy-7566bd957d-ql6qm   node-2   192.168.247.40/32    cali94236e0af24   kns.default,ksa.default.default          
node--3-k8s-sise--deploy--7566bd957d--r2pcl-eth0   sise-deploy-7566bd957d-r2pcl   node-3   192.168.139.103/32   calie4ccf73179d   kns.default,ksa.default.default

Profiles are a way to group together multiple endpoints, which will allow
 Calico to maintain things such as NetworkPolicies across the cluster.
  Let's see which profiles exist already inside of our cluster.

calicoctl get profiles -o wide
NAME                                                 LABELS                                            
ksa.kube-system.replicaset-controller                pcsa.projectcalico.org/name=replicaset-controller               
ksa.kube-system.replication-controller               pcsa.projectcalico.org/name=replication-controller
ksa.kube-system.resourcequota-controller             pcsa.projectcalico.org/name=resourcequota-controller
ksa.kube-system.service-account-controller           pcsa.projectcalico.org/name=service-account-controller           
ksa.kube-system.service-controller                   pcsa.projectcalico.org/name=service-controller                   
ksa.kube-system.statefulset-controller               pcsa.projectcalico.org/name=statefulset-controller              
ksa.kube-system.ttl-controller                       pcsa.projectcalico.org/name=ttl-controller                      
ksa.prod.default                                     pcsa.projectcalico.org/name=default                              
ksa.test.default                                     pcsa.projectcalico.org/name=default  

=====================================
Simple Calico Configuration Changes
=====================================
Now that we have explored what calicoctl can show us, let's take a look 
to see what configuration changes we can make using it.

Choose an ASN within the reserved private range: 
https://tools.ietf.org/html/rfc6996. This example will use "65103".

Number	Bits	Description	Reference
64512 - 65534	16	Reserved for private use	RFC1930, RFC6996

From https://tools.ietf.org/html/rfc1930#section-3:
An AS is a connected group of one or more IP prefixes run by one
or more network operators which has a SINGLE and CLEARLY DEFINED
routing policy.














