-------------------------------------------------------------------
Course Dates: Monday, March 1 to Friday, March 5, 2021
Course Times: 10:00 a.m. to 6:00 p.m. EST
Meeting link:
https://alta3researchinc.my.webex.com/alta3researchinc.my/j.php?MTID=m4b9a1e2b92c12e58fa1e63ef017e9e0b
Meeting number: 182 746 4400
Password: UkMK5jNik84 (85655564 from phones and video systems)

Digital copies of the course materials will be provided by your instructor during class. 
Please be prepared to access your online course materials with either the Chrome or Edge browser.
 You can test your connectivity to the lab environment by going to live.alta3.com. 
 If you see a "Welcome to Alta3 - EAS" message, you have successfully connected.

--------------------------------------------------------------------
https://live.alta3.com/groups/cheerful-crickets-burst/dashboard
sylvain
sylvain.st-germain@canada.ca

https://tmuxcheatsheet.com/
TMUX    : SHELL - Personal shell 
          split screen
          ctrl b | shift "   # Horizontal
          ctrl b | shift 5   # Vertical
          ctrl b | Arrow     # Change screen
          ctrl b | [         # scroll up  quit ->   q

TTY     : Rescue command line,
AUX1    : Take the place of localhost browser
AUX2    : Take the place of localhost browser
FiLES   : Autoindex page - Fichiers dans le répertoire static 
           student@bchd:~/static$ 

Microservice ?
Microservice architecture ?

-------------------------------------
Container 
-------------------------------------
* Put the application in isolation
* Lightweight
* Portable - self contained
* Kubernetes is a service on top of tmuxcheatsheet

-------------------------------------
Kubernetes
-------------------------------------
* External load balancer
* Cluster
    Master nodes    
    Worker node
* POds
    Wrapper
    Docker containers
    Docker registery
    Pod manifest (YAML)

Minimum - 1 node (ok for practice but not ops)
Maximum - 5000 nodes

3 nodes Minimum
3 masters - 1 is not enough even number is not good odd number absolutly

Dérange quelle noeud on roule dessus

* Possible d'avoir plus d'un cluster
* Cette semaine on jouera avec un cluster

Annoncement Kubernetes plus de support à Docker
* Docker grand pere des containers
* Kubernetes preference à Docker
* Ce qu'il ne mantienne plus est "dockershim" : cli, Volumes, API, Network, server
* Kubernetes n'a pas besoins de dockershim, on a juste besoin du container runtime
* Docker est parfaitement compatible
* No big deal !


Applications
    Monolithic
        1 executable : Database, client side, serveur side application
        Easy to start one executable
        Easy to develop
        Difficile quand plusieurs developpeur mettre des changements
        Moins agile

    SOA (service oriented applicaiton)
        Account, cart, orders - divide things
        Step up as agility
        services share one Database
        All application must speak the same language to the Database


    Microservices
        Meme chose que containers
        Data storage is isolated from containers
        Because it's not share can use different language for many containers
        Extremely agile
        Scalable

Container
    LXC 
        Base des container
    Docker
        Met un layer sur LXC pour rendre plus facile
        LXC plus un wrapper docker pour faciliter l'utilisation

PODS 
    Comment les containers peuvent parler ensemble ?
    Peut avoir des adresse IP
    Quel port est accessible sur ce Pod
    Change les rles sur comment se connecter au container

   # SEVEN - Allowed and not allowed change
    ISOLATED : CSGROUP - Resource protection
    POD : IPC - Quelle mémoire est partager, inter process commmunication
    POD : Network - network, device stack, ports,
    ISOLATED :Mount - mount point, map the directories 
    ISOLATED :PID - Processes ID
    ISOLATED :User -
    POD : UTS - container able to share hostname and domainname

Docker is just a wrapper on LXC !

LAB 1 - Docker container + Image
docker stop $(sudo docker ps -aq)
sudo docker stop $(sudo docker ps -aq)

----------------------------------------------
Plusieurs containers dans un POD

1 container de plus est creer un dummy container.
Creer individuellement, c'ets un sandbox qui fait le lienentre
tous les containers.

Il inclut les autre container dans son namespace (IPC + NETWORK + UTS)
* Tous les pod sont maintenant identifiable par la même ip adresse
* Prend sont IPC et le transfert à tous, tous les container peuvent maintenant communiquer 
par localhost, les container parlent maintenant rapidement entre eux.
* Permet à chacun des container de presenter le même hostname
* Permet à plusieurs applicaiton de fonctionner ensemble.
* Il fournissent des service à l'applicaiton principale.
* Le sandbox est pas necessaire si un pod à seulement un container
* Kubernetes mange PODS not Containers !!! Docker manage les containers
* Un POD existe seulement sur un node
* 2 PODS are the same ex:webby, ils sont des replica est comme ça ils peuvent exister
  sous forme de replica.
* Un PODS est la forme la plus atomiques de Kubernetes

Kubectl -> API serveur -> Master
Communication avec le master 
Les worker node on des kubelet (communique avec le worker)
* L'API server parle aussi au scheduler, part of master brain that play tetris : combien de resources
disponible et ou je met l'application.
* Communique avec ETC Etcd - maintain the sate of the cluster

API -> Communication HUB 

Etcd
    Key value store

Kubectl -> client pour parler au API server
kubeconfig file : file located on the local machine
        Contient la liste des cluster que l'on peut parler

Kubelet :
   Est sur les worker et master
   Recoit les ordres du master node
   Retourne le status report
   Consistently check the state of what is run inside the node
   Kubelet est reponsale de passer les commendes docker

Controller-manager
    Chef de police
    S'assure que les règles sont suivi
    ex: roule 3 replicas d'une application exemple un load balancer
    doit avoir 3 replica en tout temps
    
    Un replica meurt -> kubelet raport -> Scheduler on doit avoir 3 replica
    -> 

Docker doit rouler sur tous les nodes pour les containers dans les PODS

Registry docker.io ou local

kube-proxy : pas d,info tout de suite
    C'est sur tous les noeuds
    Essentiel pour renforcer le rles pour le traffic reseau entre les containers

--------------------------
namespaces
--------------------------
Les namespace permettre de segmenter les ressources
More precise


Manifest aller sur kubernetes et copier

---------------------------
TEMPLATES important
----------------------------


Pourquoi le kube-system à ces PODS ?
kubectl get pods -n kube-system
NAME                                       READY   STATUS    RESTARTS   AGE
calico-kube-controllers-7798c85854-kh79p   1/1     Running   0          20h
calico-node-77bj4                          1/1     Running   0          20h
calico-node-xdjhf                          1/1     Running   0          20h
calico-node-z6lbl                          1/1     Running   0          20h
coredns-5d65dd49c8-859m2                   1/1     Running   0          20h
coredns-5d65dd49c8-fz798                   1/1     Running   0          20h

Pourquoi 3 calico ?
nombre de worker node
Controller par un daemonset , s'assure qu'un pod est sur tous les nodes
Les callico pod ont les IP Adresse
calico c'est un third party apllication
Est fait pour la comunication, networking plugin

Un truc aleatoire -> replica -> calico-node-77bj4  
kubectl describe calico-node-77bj4 
Pourquoi 2 coredns ?
deux trrucs aleatoire deployment -> coredns-5d65dd49c8-859m2
Controlled by replica set
Pods that need to be alive are in deployment se recreer automatique

Replicasets
Standalon pods alone -> Bad "outside deployment"

ReplicaSet est la raison de l'implentation, maintient un set de replicas

kind: ReplicaSet
spec:
  replicas: 3

Facile d'etre scalable la nuit replicas: 8 et le jour replicas: 3

If it's broke don't fix it, recreate it ! Pods don't like change
ReplicaSet don't like change
Deployment -> Makes ReplicaSet

Deployement -> ReplicaSet -> Pods
            > New ReplicaSet > diff pods

NEVER use ReplicaSet use Deplyemnet as kind
Calico -> DaemonSet > Pods on each node

There are not load balancer they are load balancer

Calico DaemonSet ?
Deployemnet -> ReplicaSet don't care about the node but DaemonSet does care
important for Calico.

etcd : is backup by the 3 masters, replication, updated at all Times

-------------------------------
Port forwarding 
-------------------------------
kubectl port-forward PODname LocalPort:ContainerPort
kubectl port-forward nginx 2224:8080

Container port : set in Stone in the definition see on dockerhub
                 define by the image need to find it !!!

port forward is the tool to bypass le expose, access without expose
See a microservice run without putting it in production.

Exemple 2 containers, quel container sera accéder ?
webby -> 8888
nginx -> 8080

Si deux apps
webby1 8888
webby2 8888  port conflict

Lab number 9
POD has just 1 ip adresse

------------------------------
Create vs apply?
------------------------------
Imperative "create/delete"  |   Declarative "apply"
Dramatic change                 tweakin some incremental

Theoriquement on utilise seulement create ou apply, si on melanfge
les deux c'est ok mais on a des warnings.

Exec vs Attach ?
https://github.com/kubernetes/kubernetes/issues/23335

ex: Exec bash
Have access to all process

kubectl Attach
attach in a process in the container

Si un container il va direct sur le container si plus d,un container faire attention 
de specifier le bon.

First command toi debug a pod
kubectl describe pod <POD NAME>

--------------------------
lABEL + Annotation
--------------------------
Group pods together, delete, change all that pods

* In the manifest the label is under metadata
* can have many labels

kubectl label pods nginx
kubectl get pods -l environment=production,tier=frontend
kubectl get pods -l 'environment in (production),tier in (frontend)'
kubectl get pods -l 'environment in (production, qa)'
kubectl get pods -l 'environment,environment notin (frontend)'

Dans le manifest on peut faire unselecteur
 matchLabels ou matchexpression

kind: Deployment
metadata:
spec:
  selector:
    matchLabels:
     app: flaminco
     appweb : moose 

Utilise tous les pods qui match le label

# a la place de matchLabels
matchExpression:
  - { key: app, operator: IN, values: [alpaca,flamico]}

  match app: alpaca ou app: flamico

  Annotation is similar but fundamentaly different

  -------------------------------------
  Annotation :
   - Information 
   - instruction
  --------------------------------------
  Label analogie de l'etiquette sur la can de soupe
  Mais quand on regarde le label il y a beacoup d'info : ingrédient, recettes
  etc ...

  L'analogie la soupe est le label les ingrédient etc c'est les Annotations

  Annotation : release date, image info, resource lienentre
  C'est pas utiliser pour trier, juste pour l'Information

  C'est plus facile quand on a un déploiement

  Les annotation peuvent facilement cohabiter avec les labels le but est différrent

  Les annotation peuvent être utiliser comme instruction pour les autres
  objets

  Ingress : Pod (annotation with instruction to ingress to proceed)


------------------------------------
Replicaset
------------------------------------
Load balancer ?
ENvoy est utilisé dans le cluster du prof

------------------------------------
Daemon set
------------------------------------
Comment le cscheduler works ?
Cluster à 3 noeuds, le scheduler decide 
ou mettre les PODS et où il vont. Il utilise une 
technique de round-robin pour déterminer ou le pod va aller

Round-robbin faire le tour des noeud :
  - POD1 : Node 1, est-ce qu'il y a assez de resource sur ce node ? ok
  - POD2 : maintenant commence avec node 2, assez de resource ? Ok il y va
           node il va au  node 3, si 3 dit ok on le met là.
  - POD 3 : Demande au node 1 ok et va sur node 1

  Tant que les 3 pods sont faite c,est ok !

  Un daemonset est très différent, il doit avoir un POD sur chaque noeud et
  si pas assez de resource sur un noeud il y a un problème !!!

ReplicaSet : business apps
Daemonset : interne admin app, ex calico

Peut assi avoir un DaemonSet sur u certain typre d noeud ! Pas tous les noeuds
On a un selector possible mais seulement au niveau du pod dans le yaml

Two ways to make target certian node dans la section spec
nodeSelector :
  label
nodeName: node-1

git pull
kubectl create -f objets-cours/sise-ds.yaml
kubectl get pods -o wide
kubectl delete -f objets-cours/sise-ds.yaml

------------------------------------
Deployment
------------------------------------
kind and api version is the only difference between
a replicaset manifest

* expect to be change, wraper over pods
* Can also track change, versionning
* rollback !
* Keep track of 10 unique revision (can be tweak)

kubectl get pods -o wide
kubectl apply -f objets-cours/sise-deployment.yaml
kubectl get pods -o wide
kubectl delete -f objets-cours/sise-deployment.yaml

kubectl describe 

OPTINNAL SECTION 
Section "strategy" est important pour les rollback versionning
https://blog.container-solutions.com/kubernetes-deployment-strategies

recreate : create downtime

RollingUpdate : production
maxSurge: 1 #start creating new pods ex 1 one more pods
maxUnavailable : 1 # Rolling create a new pod et delete a old pod

...
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 2        # how many pods we can add at a time
      maxUnavailable: 0  # maxUnavailable define how many pods can be unavailable
                         # during the rolling update
...	

spec:
  minReadySeconds : 60 #  wait 60 secons
  progressDeadlineSeconds : 600 # Trigger a rollback if it fails


Scale object at command line
kubectl scale deploy nginx --replicas=4
or
in the manifest

Auto scale !!!
kubectl autoscale deploy nginx --min=2 --max=5 --cpu-percent=80

---------------------------------------------------
Probes - LivenessProbe + ReadinessProbes
----------------------------------------------------
* Report back on health of containers
* Liveness probe fail -> it's dead , it will kill the container
* Readiness -> container not ready yet -> block traffic to the container 
* Careful in the implementation

#Liveness
    initialDelaySeconds: 2     # wait this long after restarting to do the first HTTP GET (default 0)
      periodSeconds: 5         # how often to probe (default is 1)
      timeoutSeconds: 1        # how long to wait until timeout occurs (default is 1)
      successThreshold: 1      # min consecutive successes to be considered "up" after a fail (default 1)
      failureThreshold: 3      # how many consecutive fails before pod is restarted (default 3)
    

method  check

#1
  httpGet:                 # send HTTP GET to :9876/health/ to a endpoint pas obliger health
    path: /health
    port: 9876

#2
  exec:
    command:
    - cat 
    - /tmp/healthy

Readiness probe
    * on loading large data

Can do bothe readines + livenessProbe
https://alta3researchinc.my.webex.com/alta3researchinc.my-fr/url.php?frompanel=false&gourl=https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle

-------------------------------------
Minimu require resource
-------------------------------------
add a resources and request section

define container by containers 

...
resources: # minimum required
  requests: 
    memory: "64Mb"
    cpu: "250"
limits: # max required
  memory: "128Mb"
  cpu: "500m"
..

production -> No limit !!! no resource quota
dev        -> must be limited

resource quota : setting a hard limit in a namespace

CPU -- Compressible
Mem -- non Compressible

Eviction - a pod is ejected because the resource is not sufficient

YOu don't see request and limit on container to date.
resource quota affect namspace !

In a container to define request and limit

kubectl describe ns

LimitRange object est un objet comme resourceQuta

Alternative to resource quota

-----------------------------------
Mercredi
------------------------------------
Pourquoi les container restart, meme si pas de probe liveness ?

Dans le manifest de touts les pods par défaut
il y a une section qui est restartPolicy

restartPolicy: Always

C'est always par défaut !
Il y a d'autre option : 
onFailure - If it die on unatural death
Never - Peut être utiliser sur les container ephemere

Container are build by docker not kubernetes
What send command to docker ? Kubelet !

Auto restart is done by kubelet not kubernetes !

Containers
Dead container is rebuild by the orders of kubelet by restart policy

Pods 
rebuild by the order of the controller, if it is part of a replicaset / deployment
hence a replicaController

port-forwarding ?
Use for debugging, it's not expose the container !!! different

kubectl port-forward pod localport:containerPort

What are label use for ?
grouping similar objet -> YES
determing what pods belong to object like deployement -> YES
Providing metadata to object -> No, it's annotation

Garantee the container has a certain amount of resources ?
containers:
  resources:
    requests:

--------------------------------
kubectl create ns demo 

kubectl describe ns demo

kubectl create -f pod.yaml -n demo

#Creatioj d'un objet LimitRange 
# Per name space s'applique a tous les pods

kubectl create -f limitrange.yaml -n demo
kubeclt describe ns demo

# cree un pod "pod" dans le namespace demo 
kubeclt create -f pod.yaml -n demo
kubectl describe pod -n demo pod | grep Requests -A 2
kubectl describe pod -n demo pod | grep Limits -A 2

ResourceQuota set a hard limit to 60Mi by example

What happen to pod already there ????
kubeclt describe ns demo

PODS attribute
limits, requests, 

Object 
ResourceQuota
LimitRange

J'ai fini lab20 !               
------------------------------
Manqué 40 mnutes
-------------------------------
Crontab

crontab guru webpage
https://crontab.guru/

Lab create a cronjob

cronjob -> job -> pod -> container -> task in the cluster

# parametre important
parallelism
completion 

jenkins (lab 32)
is a gui that can create job in a kubernetes cluster

how to suspend cronjob ?
kubectl edit cj    # change in it


-----------------------------------------------
Persistent storage 
ConfigMaps & Secrets
------------------------------------------------

Step 1 - pod
create a config map : anifest pour le POD avec nginx

step 2 - create the configuration file
create a nginx.conf

step 3 - create a config map object
                         nom significaif
kubectl create configmap nginx-conf --from-file=nginx.conf 

on met le fichier nginx.conf dans le config map nginx-conf

kubectl get cm 
kubectl describe cm 

step 4 - include the configMap object in the manifest (in the pod)

La configmap est independante du container et est contenu dans etcd !!!!

tous les containers obtienne le meme mountpoint vers le configmap dans etcd
c'est au namespace level - c'est circonscript par un namespace !!


Info
Binary data cannot be in a copnfigMap only ascii file

Maintain ?

How we can take a config map ?
kubeclt get cm

How to mount an object like a configmap in a container inside a POD.
nginx pod (nginx.yaml)

configMap : nginx-config
the map has the file nginx.conf

Before I can put a configmap in the container, I need to build a library inside the pod.


If the directory is not existing it will create it
If the directory exist it will overwrite


Besoins aussi des configMap pour les password etc ..
ca s.appel un "secret"  un configMap secure

Lot of kind of sercret but generic is fine
limited by namescape !

-----------------------------------------
Container initial
------------------------------------------
kubectl get svc
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   172.16.3.1   <none>        443/TCP   2d2h

kubectl expose pod nginx-locked-n-loaded
service/nginx-locked-n-loaded exposed

NAME                    TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)          AGE
kubernetes              ClusterIP   172.16.3.1    <none>        443/TCP          2d2h
nginx-locked-n-loaded   ClusterIP   172.16.3.69   <none>        80/TCP,443/TCP   14s

kubectl describe svc nginx-locked-n-loaded

Name:              nginx-locked-n-loaded
Namespace:         default
Labels:            app=nginx-configured
Annotations:       <none>
Selector:          app=nginx-configured
Type:              ClusterIP
IP:                172.16.3.69
Port:              port-1  80/TCP
TargetPort:        80/TCP
Endpoints:         192.168.139.101:80,192.168.247.37:80
Port:              port-2  443/TCP
TargetPort:        443/TCP
Endpoints:         192.168.139.101:443,192.168.247.37:443
Session Affinity:  None
Events:            <none>

kubectl get pod -o wide
NAME                            READY   STATUS    RESTARTS   AGE   IP                NODE     NOMINATED NODE   READINESS GATES
nginx-locked-n-loaded           1/1     Running   0          27m   192.168.139.101   node-3   <none>           <none>
nginx-locked-n-loaded-secrets   1/1     Running   0          19m   192.168.247.37    node-2   <none>           <none>

nginx-locked-n-loaded : the IP match the endpoint !!!

create a service !!!
Keep an eye on the pod

Expose a deployment !!!
kubectl expose deploy zombie

On a deployement all IP is expose !

Problem every time a pod is recreate the pod has a new IP address
be fortunately ! The service keep track and adapt to the new IP

A service is need to enable microservice an make container able to speak together

Object : StorageClass

CEPHFS est utilisé pour 

-----------------------
NFS not high available 
-----------------------
node-1 : NFS server
node-2 : NFS-client
node-3 : NFS-client

StorageClass
persitent volume is separate alone can live without pod !

19 diffewrent kind of storage, and they are very different !!
All of them has a reclaim Policy 

https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner


Le storage PV faire attention sur quel noeud il est fait !!!

CEPHFS ou RBD  -> need both
microservices pa


==================================================
NETWORKS (4 hops !!!)
==================================================
1# IP Cluster (loadbalancer) external facing public IP
kubectl get services -o wide
load balance to nodes

2# IP Nodes (10.0.0.0)
kubectl get nodes -o wide
NAME     STATUS   ROLES    AGE     VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION       CONTAINER-RUNTIME
node-1   Ready    <none>   2d21h   v1.18.0   10.3.188.245   <none>        Ubuntu 18.04.5 LTS   4.15.0-136-generic   containerd://1.3.3
node-2   Ready    <none>   2d21h   v1.18.0   10.9.54.194    <none>        Ubuntu 18.04.5 LTS   4.15.0-136-generic   containerd://1.3.3
node-3   Ready    <none>   2d21h   v1.18.0   10.8.79.30     <none>        Ubuntu 18.04.5 LTS   4.15.0-136-generic   containerd://1.3.3
theres a Kubeproxy that load balance between pod

3# IP of service (172.16.0.0)
kubectl get service -o wide
kubectl describe 

NAME            TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE    SELECTOR
kubernetes      ClusterIP   172.16.3.1     <none>        443/TCP   137m   <none>
simpleservice   ClusterIP   172.16.3.158   <none>        80/TCP    133m   app=sise

kubectl describe svc -n test
   - clusterIP
   -
4# POD IP (192.168.0.0)
NAME                           READY   STATUS    RESTARTS   AGE    IP                NODE     NOMINATED NODE   READINESS GATES
sise-deploy-7566bd957d-ql6qm   1/1     Running   0          134m   192.168.247.40    node-2   <none>           <none>
sise-deploy-7566bd957d-r2pcl   1/1     Running   0          134m   192.168.139.103   node-3   <none>           <none>


30000-32767 -> Node port acces to serivce a pointer to services

INGRESS- incoming connections to the Pod 
EGRESS- outgoing connections from the Pod 
REPLICAS- identical clones of a Pod 
ENDPOINT- the IP address of a Pod 
SERVICE- created when a set of Pods are exposed; it is a pointer to all of those Pods' 
IP addresses. 
LOAD BALANCER- used to balance traffic round-robin across all nodes in the cluster. 
NODE IP- the IP address of a node. 
NODE PORT- a port that is opened on every node of your cluster, allowing access to the service that you need. Always between 30000-32767. 
CLUSTER IP SUBNET- all the ClusterIP services in the cluster. 
KUBE PROXY- located on every node; interprets network rules that allow access to the Cluster IP. Also enables load-balancing across replicated Pods.

Cluster IP vs Nodeport service ?
Under Type 

# Comprendre calkico sur le namaspace kube-system
kubectl get pods -n kube-system
NAME                                       READY   STATUS    RESTARTS   AGE
calico-kube-controllers-7798c85854-kh79p   1/1     Running   0          2d22h
calico-node-77bj4                          1/1     Running   0          2d22h
calico-node-xdjhf                          1/1     Running   0          2d22h
calico-node-z6lbl                          1/1     Running   0          2d22h
calicoctl                                  1/1     Running   0          176m
coredns-5d65dd49c8-859m2                   1/1     Running   0          2d22h
coredns-5d65dd49c8-fz798                   1/1     Running   0          2d22h

-------------------------------
How Calicco plugin work ?
--------------------------------
kubectl get pods -n kube-system -o wide
NAME                                       READY   STATUS    RESTARTS   AGE     IP               NODE     NOMINATED NODE   READINESS GATES
calico-kube-controllers-7798c85854-kh79p   1/1     Running   0          2d23h   10.3.188.245     node-1   <none>           <none>
calico-node-77bj4                          1/1     Running   0          2d23h   10.3.188.245     node-1   <none>           <none>
calico-node-xdjhf                          1/1     Running   0          2d23h   10.9.54.194      node-2   <none>           <none>
calico-node-z6lbl                          1/1     Running   0          2d23h   10.8.79.30       node-3   <none>           <none>
calicoctl                                  1/1     Running   0          3h15m   10.8.79.30       node-3   <none>           <none>
coredns-5d65dd49c8-859m2                   1/1     Running   0          2d23h   192.168.139.66   node-3   <none>           <none>
coredns-5d65dd49c8-fz798                   1/1     Running   0          2d23h   192.168.139.65   node-3   <none>           <none>


=============================================
Ingress and Ingress controller
=============================================
* Object the sit on top of services
* Replace Nodeport
* GIve a nodeport to ingress controller
* Can be 1 nodeport by "batch of services" instead of one
* Not natively install 
























































